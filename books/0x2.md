## Pyhacker 之 网站爬虫

**00x1：**  

需要用到的模块如下：  

import requests  

import re  

**00x2：**  

网站爬虫分类：  

1.搜索引擎 inurl:hackxc.cc  

2.正则匹配href内的url  

3.目录扫描爬虫探测  

4.爬取js，分析js内的url，爬取隐藏接口  

5.更多的py爬虫模块  

**00x3：**  

这里我们在前面实现了第一个功能，后面还有目录扫描，所以我们主要讲，第二个方法，其他都是大同小异  

以www.hackxc.cc 为例  

首先提取href内的url，遍历继续访问提取的url，反复如此  

```
import urllib3
urllib3.disable_warnings()
req = requests.get(url,headers=headers,timeout=3,verify=False)
```  

![img](https://github.com/hackxc/Pyhacker/blob/master/books/img/2/1.png)  

href="http://www.hackxc.cc/?post=4" >  

正则为：href=”(.*?)”  

![img](https://github.com/hackxc/Pyhacker/blob/master/books/img/2/2.png)  

没毛病，但是还有很多我们不想要的url  

这里大家根据自己需求进行修改  

比如，我只想要hackxc.cc 域名下的url，排除其他url  

![img](https://github.com/hackxc/Pyhacker/blob/master/books/img/2/3.png)  

当然不止是href内的url，还有src标签下的url  

这里仅是抛砖引玉，原理同上  

有重复项  

![img](https://github.com/hackxc/Pyhacker/blob/master/books/img/2/4.png)  

我们再来一个去重复  

```
for url in urls:
    if 'hackxc.cc' in url:
        if url not in url_s:
            url_s.append(url)
```  

已去除

![img](https://github.com/hackxc/Pyhacker/blob/master/books/img/2/5.png)  

当然这里还有css等文件，我们可以继续判断，剔除  

也可以写个列表遍历判断剔除  

简单处理一下 css  

![img](https://github.com/hackxc/Pyhacker/blob/master/books/img/2/6.png)  

**00x4：**  

接下来我们来遍历这个url_s  

继续爬行  

```
print u"开始第一轮采集"
depth('http://www.hackxc.cc/?post=21')
print u"采集到了%s url"%len(url_s)

print u"开始执行第二次采集"
for url in url_s:
    depth(url)
    print url

print u"采集到了%s url"%len(url_s)

print u"采集完成：%s"%url_s
```  

**00x5：**  

为了更加智能化，我们修改刚才设置的域名的地方  

![img](https://github.com/hackxc/Pyhacker/blob/master/books/img/2/7.png)  

![img](https://github.com/hackxc/Pyhacker/blob/master/books/img/2/8.png)  

```
def main():
    url =raw_input('\nurl:')
    #取域名部分
    r = url.split('.')
    who = r[-2]+"."+r[-1]
    if 'http' not in url:
        url = 'http://'+url
    depth(url,who)
    print u"开始第一轮采集,采集到了%s url"%len(url_s)

    print u"开始执行第二次采集"
    for url in url_s:
        depth(url,who)
        print url
    print u"采集到了%s url"%len(url_s)
    print u"采集完成：%s"%url_s
```  

**00x6：**  

循环深度爬行完善代码  

```
def main():
    url =raw_input('\nurl:')
    #取域名部分
    r = url.split('.')
    who = r[-2]+"."+r[-1]
    if 'http' not in url:
        url = 'http://'+url
    depth(url,who)
    print u"\n采集到了%s url"%len(url_s)

    while True:
        print u"\n开始执行深度采集"
        for url in url_s:
            if 'http' not in url:
                url = 'http://' + url
            depth(url,who)
            print url
        if len(url_s)==len(url_s):
            break
```  

现在我们来测试一下  

首先我多加了两个url，他会爬去到/qc  

![img](https://github.com/hackxc/Pyhacker/blob/master/books/img/2/9.png)  

在/qc，我又加了一个url，看看他能不能爬去到  

![img](https://github.com/hackxc/Pyhacker/blob/master/books/img/2/10.png)  

看看他是不是可以一直深度爬取url  

1.  

先爬去tools.hackxc.cc  

只有一个www.hackxc.cc  

2.  

接着从www.hackxc.cc  

爬到tools.hackxc.cc/qc  

3.  

再从tools.hackxc.cc/qc  

爬行到tools.hackxc.cc/test!!!!!  

![img](https://github.com/hackxc/Pyhacker/blob/master/books/img/2/11.png)  

OK，一切正常  

**00x7：**  

下面我们让他自动保存即可  

```
with open('Url_depth.txt','a+')as f:
    for url in url_s:
        f.write(url+"\n")
```  

**00x8：**  

完整代码：/books/config/2.[Pyhacker]网站爬虫  

```
#!/usr/bin/python
#-*- coding:utf-8 -*-
import requests
import re
import urllib3
urllib3.disable_warnings()

url_s = []

def depth(url,who):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3314.0 Safari/537.36 SE 2.X MetaSr 1.0'}
        req = requests.get(url=url,headers=headers,timeout=3,verify=False)
        r = r'href="(.*?)"'
        urls = re.findall(r,req.content)
        for url in urls:
            if who in url:
                if url not in url_s:
                    if 'css' not in url :
                        url_s.append(url)
    except:
        pass

def main():
    url =raw_input('\nurl:')
    #取域名部分
    r = url.split('.')
    who = r[-2]+"."+r[-1]
    if 'http' not in url:
        url = 'http://'+url
    depth(url,who)
    print u"\n采集到了%s url"%len(url_s)

    while True:
        print u"\n开始执行深度采集"
        for url in url_s:
            if 'http' not in url:
                url = 'http://' + url
            depth(url,who)
            print url
        if len(url_s)==len(url_s):
            break

if __name__ == '__main__':
    main()
    print u"\n采集完成"
    print u"\n采集到了%surl"%len(url_s)
    with open('Url_depth.txt','a+')as f:
        for url in url_s:
            f.write(url+"\n")
    print "\nSave the url_depth.txt!!!"
```